# -*- coding: utf-8 -*-
"""Another copy of Fork of ISIC Training 08/29/24 b9a6ef

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NuG6ylOfQ8gvakzBbfasLlPqRzTrQXLg
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'isic-2024-challenge:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F63056%2F9094797%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240905%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240905T201432Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D679a582e7c2d83bb3e93ff1137bd105cb39794cbfb6cdf03baa4bb9b324a26773bebc4c0f205e63cf6144d23cdb5ee218de340d1bad317d38e537815e6224ddd5fb5fe5451430837d954fdf710aa674ee1d0e79b4bdb68cbaf752562aa0d5a467dfc3acb26d9f287b51c1af155ee1ecefb2aff7680d7c1e1dc813108820a9898f9154d5423f7ef35b0eb0d97631bf4988e8776d2eef553e1147ae7a77e62ecfb6d576cc92414e25d623bb46b787cfe472e7fe13a7de680652b0e582da3946526898debab82b1f65bfbbaac0b4e181a3c22923fa5b5b809db952e490647e7bb687bff18cfbdff6dd9357d827bcc8b1410a847c04816403b250d51a89cee785375'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

!pip install wandb
!pip install accelerate -U
!pip install transformers[torch]
!pip install timm
!pip install kornia
!pip install lightgbm
!pip install catboost
!pip install xgboost

import pandas as pd
import numpy as np
import h5py
from tqdm.notebook import tqdm, trange
from PIL import Image
import io
from sklearn.metrics import roc_curve, auc, roc_auc_score, log_loss
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader, Sampler
import torch.nn as nn
import torch
import albumentations as A
from sklearn.model_selection import StratifiedKFold, StratifiedGroupKFold
from transformers import EvalPrediction, TrainerCallback, AutoImageProcessor, ResNetModel, ViTModel, ViTForImageClassification, EfficientNetModel, TrainingArguments, Trainer, EfficientNetForImageClassification
from transformers.optimization import Adafactor, get_cosine_schedule_with_warmup, get_cosine_with_hard_restarts_schedule_with_warmup, AdamW
import os
import random
from sklearn.impute import SimpleImputer
from torchvision.transforms.v2 import (
    CenterCrop,
    Compose,
    Lambda,
    Normalize,
    RandomHorizontalFlip,
    RandomVerticalFlip,
    RandomZoomOut,
    RandomResizedCrop,
    RandomAdjustSharpness,
    RandomAutocontrast,
    Resize,
    ToTensor,
    ToPILImage,
    ColorJitter,
    RandomRotation,
    RandomPerspective,
    GaussianBlur,
    InterpolationMode,
    RandomAffine,
)
from io import BytesIO

from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts
from pathlib import Path
import timm
from transformers.modeling_outputs import SequenceClassifierOutput
import math
import cv2
from imblearn.over_sampling import RandomOverSampler, SMOTE
from imblearn.under_sampling import RandomUnderSampler
from typing import Callable
import torch.nn.functional as t_func
from kornia import image_to_tensor, tensor_to_image
import kornia.augmentation as K
import kornia.filters as F
import kornia.geometry.transform as G
import cv2
from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder, OrdinalEncoder
import kornia.color as kc
import kornia.morphology as km
from imblearn.over_sampling import RandomOverSampler
import polars as pl
import gc
import safetensors
import warnings

warnings.filterwarnings("ignore", category=FutureWarning)
tqdm.pandas()

def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed(0)

class config:
    wandb = True
    debug = False
    fold = 3
    lr = 5e-4
    train_batch_size=32
    valid_batch_size=128
    epochs=2
    device="cuda" if torch.cuda.is_available() else "cpu"
    sharpness_factor=0.8
    model_pth="tf_efficientnet_b3_ns"
    data_downloaded=False
    embedding_size=512
    warmup_fraction=0.05
    data_pth="/kaggle/input/isic-2024-challenge"
    beta=0.1
    vh_mixup = True
    balanced_mixup=True
    size=(384, 384)
    contrastive_learning=False
    projection_dim=1024
    lambda_loss=0.5
    pretrain=False
    manifold_mixup=False
    temperature = 0.07

if config.wandb:
    import wandb
    os.environ["WANDB_DISABLED"] = "false"
    os.environ["WANDB_RUN_GROUP"] = "experiment-" + wandb.util.generate_id()
    wandb.login(key="f6be3c22658b12b25148ebe62e8c23968d67b1e2")
else:
    os.environ["WANDB_DISABLED"] = "true"

id_col = 'isic_id'
target_col = 'target'
group_col = 'patient_id'

err = 1e-5
sampling_ratio = 0.01
seed = 42

num_cols = [
    'age_approx',                        # Approximate age of patient at time of imaging.
    'clin_size_long_diam_mm',            # Maximum diameter of the lesion (mm).+
    'tbp_lv_A',                          # A inside  lesion.+
    'tbp_lv_Aext',                       # A outside lesion.+
    'tbp_lv_B',                          # B inside  lesion.+
    'tbp_lv_Bext',                       # B outside lesion.+
    'tbp_lv_C',                          # Chroma inside  lesion.+
    'tbp_lv_Cext',                       # Chroma outside lesion.+
    'tbp_lv_H',                          # Hue inside the lesion; calculated as the angle of A* and B* in LAB* color space. Typical values range from 25 (red) to 75 (brown).+
    'tbp_lv_Hext',                       # Hue outside lesion.+
    'tbp_lv_L',                          # L inside lesion.+
    'tbp_lv_Lext',                       # L outside lesion.+
    'tbp_lv_areaMM2',                    # Area of lesion (mm^2).+
    'tbp_lv_area_perim_ratio',           # Border jaggedness, the ratio between lesions perimeter and area. Circular lesions will have low values; irregular shaped lesions will have higher values. Values range 0-10.+
    'tbp_lv_color_std_mean',             # Color irregularity, calculated as the variance of colors within the lesion's boundary.
    'tbp_lv_deltaA',                     # Average A contrast (inside vs. outside lesion).+
    'tbp_lv_deltaB',                     # Average B contrast (inside vs. outside lesion).+
    'tbp_lv_deltaL',                     # Average L contrast (inside vs. outside lesion).+
    'tbp_lv_deltaLB',                    #
    'tbp_lv_deltaLBnorm',                # Contrast between the lesion and its immediate surrounding skin. Low contrast lesions tend to be faintly visible such as freckles; high contrast lesions tend to be those with darker pigment. Calculated as the average delta LB of the lesion relative to its immediate background in LAB* color space. Typical values range from 5.5 to 25.+
    'tbp_lv_eccentricity',               # Eccentricity.+
    'tbp_lv_minorAxisMM',                # Smallest lesion diameter (mm).+
    'tbp_lv_nevi_confidence',            # Nevus confidence score (0-100 scale) is a convolutional neural network classifier estimated probability that the lesion is a nevus. The neural network was trained on approximately 57,000 lesions that were classified and labeled by a dermatologist.+,++
    'tbp_lv_norm_border',                # Border irregularity (0-10 scale); the normalized average of border jaggedness and asymmetry.+
    'tbp_lv_norm_color',                 # Color variation (0-10 scale); the normalized average of color asymmetry and color irregularity.+
    'tbp_lv_perimeterMM',                # Perimeter of lesion (mm).+
    'tbp_lv_radial_color_std_max',       # Color asymmetry, a measure of asymmetry of the spatial distribution of color within the lesion. This score is calculated by looking at the average standard deviation in LAB* color space within concentric rings originating from the lesion center. Values range 0-10.+
    'tbp_lv_stdL',                       # Standard deviation of L inside  lesion.+
    'tbp_lv_stdLExt',                    # Standard deviation of L outside lesion.+
    'tbp_lv_symm_2axis',                 # Border asymmetry; a measure of asymmetry of the lesion's contour about an axis perpendicular to the lesion's most symmetric axis. Lesions with two axes of symmetry will therefore have low scores (more symmetric), while lesions with only one or zero axes of symmetry will have higher scores (less symmetric). This score is calculated by comparing opposite halves of the lesion contour over many degrees of rotation. The angle where the halves are most similar identifies the principal axis of symmetry, while the second axis of symmetry is perpendicular to the principal axis. Border asymmetry is reported as the asymmetry value about this second axis. Values range 0-10.+
    'tbp_lv_symm_2axis_angle',           # Lesion border asymmetry angle.+
    'tbp_lv_x',                          # X-coordinate of the lesion on 3D TBP.+
    'tbp_lv_y',                          # Y-coordinate of the lesion on 3D TBP.+
    'tbp_lv_z',                          # Z-coordinate of the lesion on 3D TBP.+
]

new_num_cols = [
    'lesion_size_ratio',             # tbp_lv_minorAxisMM      / clin_size_long_diam_mm
    'lesion_shape_index',            # tbp_lv_areaMM2          / tbp_lv_perimeterMM **2
    'hue_contrast',                  # tbp_lv_H                - tbp_lv_Hext              abs
    'luminance_contrast',            # tbp_lv_L                - tbp_lv_Lext              abs
    'lesion_color_difference',       # tbp_lv_deltaA **2       + tbp_lv_deltaB **2 + tbp_lv_deltaL **2  sqrt
    'border_complexity',             # tbp_lv_norm_border      + tbp_lv_symm_2axis
    'color_uniformity',              # tbp_lv_color_std_mean   / tbp_lv_radial_color_std_max

    'position_distance_3d',          # tbp_lv_x **2 + tbp_lv_y **2 + tbp_lv_z **2  sqrt
    'perimeter_to_area_ratio',       # tbp_lv_perimeterMM      / tbp_lv_areaMM2
    'area_to_perimeter_ratio',       # tbp_lv_areaMM2          / tbp_lv_perimeterMM
    'lesion_visibility_score',       # tbp_lv_deltaLBnorm      + tbp_lv_norm_color
    'symmetry_border_consistency',   # tbp_lv_symm_2axis       * tbp_lv_norm_border
    'consistency_symmetry_border',   # tbp_lv_symm_2axis       * tbp_lv_norm_border / (tbp_lv_symm_2axis + tbp_lv_norm_border)

    'color_consistency',             # tbp_lv_stdL             / tbp_lv_Lext
    'consistency_color',             # tbp_lv_stdL*tbp_lv_Lext / tbp_lv_stdL + tbp_lv_Lext
    'size_age_interaction',          # clin_size_long_diam_mm  * age_approx
    'hue_color_std_interaction',     # tbp_lv_H                * tbp_lv_color_std_mean
    'lesion_severity_index',         # tbp_lv_norm_border      + tbp_lv_norm_color + tbp_lv_eccentricity / 3
    'shape_complexity_index',        # border_complexity       + lesion_shape_index
    'color_contrast_index',          # tbp_lv_deltaA + tbp_lv_deltaB + tbp_lv_deltaL + tbp_lv_deltaLBnorm

    'log_lesion_area',               # tbp_lv_areaMM2          + 1  np.log
    'normalized_lesion_size',        # clin_size_long_diam_mm  / age_approx
    'mean_hue_difference',           # tbp_lv_H                + tbp_lv_Hext    / 2
    'std_dev_contrast',              # tbp_lv_deltaA **2 + tbp_lv_deltaB **2 + tbp_lv_deltaL **2   / 3  np.sqrt
    'color_shape_composite_index',   # tbp_lv_color_std_mean   + bp_lv_area_perim_ratio + tbp_lv_symm_2axis   / 3
    'lesion_orientation_3d',         # tbp_lv_y                , tbp_lv_x  np.arctan2
    'overall_color_difference',      # tbp_lv_deltaA           + tbp_lv_deltaB + tbp_lv_deltaL   / 3

    'symmetry_perimeter_interaction',# tbp_lv_symm_2axis       * tbp_lv_perimeterMM
    'comprehensive_lesion_index',    # tbp_lv_area_perim_ratio + tbp_lv_eccentricity + bp_lv_norm_color + tbp_lv_symm_2axis   / 4
    'color_variance_ratio',          # tbp_lv_color_std_mean   / tbp_lv_stdLExt,
    'border_color_interaction',      # tbp_lv_norm_border      * tbp_lv_norm_color
    'border_color_interaction_2',
    'size_color_contrast_ratio',     # clin_size_long_diam_mm  / tbp_lv_deltaLBnorm
    'age_normalized_nevi_confidence',# tbp_lv_nevi_confidence  / age_approx
    'age_normalized_nevi_confidence_2',
    'color_asymmetry_index',         # tbp_lv_symm_2axis       * tbp_lv_radial_color_std_max

    'volume_approximation_3d',       # tbp_lv_areaMM2          * sqrt(tbp_lv_x**2 + tbp_lv_y**2 + tbp_lv_z**2)
    'color_range',                   # abs(tbp_lv_L - tbp_lv_Lext) + abs(tbp_lv_A - tbp_lv_Aext) + abs(tbp_lv_B - tbp_lv_Bext)
    'shape_color_consistency',       # tbp_lv_eccentricity     * tbp_lv_color_std_mean
    'border_length_ratio',           # tbp_lv_perimeterMM      / pi * sqrt(tbp_lv_areaMM2 / pi)
    'age_size_symmetry_index',       # age_approx              * clin_size_long_diam_mm * tbp_lv_symm_2axis
    'index_age_size_symmetry',
    "circularity",
    "eccentricity_to_area",
    "avg_color_diff",
    "avg_color_std",
    "color_asymmetry_ratio",
    "border_irregularity_enhanced"# age_approx              * tbp_lv_areaMM2 * tbp_lv_symm_2axis
] + [
    'lesion_eccentricity_perimeter_ratio',
    'hue_to_luminance_ratio',
    'size_severity_index',
    'border_area_interaction',
    'elliptical_asymmetry',
    'compactness',
    'average_chroma',
    'texture_variation',
    'border_chroma_interaction',
    '3d_orientation_diff',
    '3d_position_weighted_area',
    'color_shape_severity_index',
    'combined_complexity_index',
    'overall_complexity_score',
    'median_color_difference',
    'range_of_hue',
    'skewness_of_color_distribution',
    'age_standardized_color_variation',
    'patient_mean_severity',
    'log_color_contrast_index',
    'square_root_perimeter'
]

cat_cols = ['sex', 'anatom_site_general', 'tbp_tile_type', 'tbp_lv_location', 'tbp_lv_location_simple', 'attribution']
norm_cols = [f'{col}_progression' for col in num_cols + new_num_cols] + [f'{col}_patient_norm' for col in num_cols + new_num_cols] + [f'{col}_patient_mean' for col in num_cols + new_num_cols] + [f'{col}_patient_std' for col in num_cols + new_num_cols] + [f'{col}_patient_min' for col in num_cols + new_num_cols] + [f'{col}_patient_max' for col in num_cols + new_num_cols] + [f'{col}_patient_median' for col in num_cols + new_num_cols]
special_cols = ['count_per_patient']
feature_cols = num_cols + new_num_cols + cat_cols + norm_cols + special_cols

def read_data(path):
    # Read CSV data into a DataFrame
    df = pd.read_csv(path)

    for col in df.columns:
        df[col] = df[col].replace('NA', np.nan)

    # Impute missing values with median for numeric columns
    cols = df.select_dtypes(include=[np.float32, np.int32, np.int64, np.float64]).columns
    df[cols] = df[cols].fillna(df[cols].median())

    # Feature engineering
    df['lesion_size_ratio'] = df['tbp_lv_minorAxisMM'] / (df['clin_size_long_diam_mm'] + 1e-8)
    df['lesion_shape_index'] = df['tbp_lv_areaMM2'] / (df['tbp_lv_perimeterMM'] ** 2  + 1e-8)
    df['hue_contrast'] = abs(df['tbp_lv_H'] - df['tbp_lv_Hext'])
    df['luminance_contrast'] = abs(df['tbp_lv_L'] - df['tbp_lv_Lext'])
    df['lesion_color_difference'] = np.sqrt(df['tbp_lv_deltaA'] ** 2 + df['tbp_lv_deltaB'] ** 2 + df['tbp_lv_deltaL'] ** 2)
    df['border_complexity'] = df['tbp_lv_norm_border'] + df['tbp_lv_symm_2axis']
    df['color_uniformity'] = df['tbp_lv_color_std_mean'] / (df['tbp_lv_radial_color_std_max'] + 1e-8)  # Small epsilon to avoid division by zero

    # Additional feature engineering
    df['position_distance_3d'] = np.sqrt(df['tbp_lv_x'] ** 2 + df['tbp_lv_y'] ** 2 + df['tbp_lv_z'] ** 2)
    df['perimeter_to_area_ratio'] = df['tbp_lv_perimeterMM'] / (df['tbp_lv_areaMM2'] + 1e-8)
    df['area_to_perimeter_ratio'] = df['tbp_lv_areaMM2'] / (df['tbp_lv_perimeterMM'] + 1e-8)
    df['lesion_visibility_score'] = df['tbp_lv_deltaLBnorm'] + df['tbp_lv_norm_color']
    df['combined_anatomical_site'] = df['anatom_site_general'] + '_' + df['tbp_lv_location']
    df['symmetry_border_consistency'] = df['tbp_lv_symm_2axis'] * df['tbp_lv_norm_border']
    df['consistency_symmetry_border'] = df['tbp_lv_symm_2axis'] * df['tbp_lv_norm_border'] / (df['tbp_lv_symm_2axis'] + df['tbp_lv_norm_border'] + 1e-8)  # Small epsilon

    # More features
    df['color_consistency'] = df['tbp_lv_stdL'] / df['tbp_lv_Lext'] + 1e-8
    df['consistency_color'] = df['tbp_lv_stdL'] * df['tbp_lv_Lext'] / (df['tbp_lv_stdL'] + df['tbp_lv_Lext'] + 1e-8)  # Small epsilon
    df['size_age_interaction'] = df['clin_size_long_diam_mm'] * df['age_approx']
    df['hue_color_std_interaction'] = df['tbp_lv_H'] * df['tbp_lv_color_std_mean']
    df['lesion_severity_index'] = (df['tbp_lv_norm_border'] + df['tbp_lv_norm_color'] + df['tbp_lv_eccentricity']) / 3
    df['shape_complexity_index'] = df['border_complexity'] + df['lesion_shape_index']
    df['color_contrast_index'] = df['tbp_lv_deltaA'] + df['tbp_lv_deltaB'] + df['tbp_lv_deltaL'] + df['tbp_lv_deltaLBnorm']

    df['log_lesion_area'] = np.log(df['tbp_lv_areaMM2'] + 1)
    df['normalized_lesion_size'] = df['clin_size_long_diam_mm'] / (df['age_approx'] + 1e-8)
    df['mean_hue_difference'] = (df['tbp_lv_H'] + df['tbp_lv_Hext']) / 2
    df['std_dev_contrast'] = np.sqrt((df['tbp_lv_deltaA'] ** 2 + df['tbp_lv_deltaB'] ** 2 + df['tbp_lv_deltaL'] ** 2) / 3)
    df['color_shape_composite_index'] = (df['tbp_lv_color_std_mean'] + df['tbp_lv_area_perim_ratio'] + df['tbp_lv_symm_2axis']) / 3
    df['lesion_orientation_3d'] = np.arctan2(df['tbp_lv_y'], df['tbp_lv_x'])
    df['overall_color_difference'] = (df['tbp_lv_deltaA'] + df['tbp_lv_deltaB'] + df['tbp_lv_deltaL']) / 3

    df['symmetry_perimeter_interaction'] = df['tbp_lv_symm_2axis'] * df['tbp_lv_perimeterMM']
    df['comprehensive_lesion_index'] = (df['tbp_lv_area_perim_ratio'] + df['tbp_lv_eccentricity'] + df['tbp_lv_norm_color'] + df['tbp_lv_symm_2axis']) / 4
    df['color_variance_ratio'] = df['tbp_lv_color_std_mean'] / (df['tbp_lv_stdLExt'] + 1e-8)
    df['border_color_interaction'] = df['tbp_lv_norm_border'] * df['tbp_lv_norm_color']
    df['border_color_interaction_2'] = df['tbp_lv_norm_border'] * df['tbp_lv_norm_color'] / (df['tbp_lv_norm_border'] + df['tbp_lv_norm_color'] + 1e-8)  # Small epsilon
    df['size_color_contrast_ratio'] = df['clin_size_long_diam_mm'] / (df['tbp_lv_deltaLBnorm'] + 1e-8)
    df['age_normalized_nevi_confidence'] = df['tbp_lv_nevi_confidence'] / (df['age_approx'] + 1e-8)
    df['age_normalized_nevi_confidence_2'] = np.sqrt(df['clin_size_long_diam_mm']**2 + df['age_approx']**2)
    df['color_asymmetry_index'] = df['tbp_lv_radial_color_std_max'] * df['tbp_lv_symm_2axis']

    df['volume_approximation_3d'] = df['tbp_lv_areaMM2'] * np.sqrt(df['tbp_lv_x']**2 + df['tbp_lv_y']**2 + df['tbp_lv_z']**2)
    df['color_range'] = abs(df['tbp_lv_L'] - df['tbp_lv_Lext']) + abs(df['tbp_lv_A'] - df['tbp_lv_Aext']) + abs(df['tbp_lv_B'] - df['tbp_lv_Bext'])
    df['shape_color_consistency'] = df['tbp_lv_eccentricity'] * df['tbp_lv_color_std_mean']
    df['border_length_ratio'] = df['tbp_lv_perimeterMM'] / (2 * np.pi * np.sqrt(df['tbp_lv_areaMM2'] / np.pi) + 1e-8)
    df['age_size_symmetry_index'] = df['age_approx'] * df['clin_size_long_diam_mm'] * df['tbp_lv_symm_2axis']
    df['index_age_size_symmetry'] = df['age_approx'] * df['tbp_lv_areaMM2'] * df['tbp_lv_symm_2axis']

    df["circularity"] = (4 * np.pi * df["tbp_lv_areaMM2"]) / (df["tbp_lv_perimeterMM"] ** 2 + 1e-8)
    df['eccentricity_to_area'] = df['tbp_lv_eccentricity'] / (df['tbp_lv_areaMM2'] + 1e-8)
    df['avg_color_diff'] = df[['tbp_lv_deltaA', 'tbp_lv_deltaB', 'tbp_lv_deltaL']].mean(axis=1)
    df['avg_color_std'] = df[['tbp_lv_deltaA', 'tbp_lv_deltaB', 'tbp_lv_deltaL']].std(axis=1)
    df['color_asymmetry_ratio'] = df['tbp_lv_radial_color_std_max'] / (df['tbp_lv_stdL'] + 1e-8)
    df['border_irregularity_enhanced'] = df['tbp_lv_norm_border'] * df['tbp_lv_area_perim_ratio']

    df['lesion_eccentricity_perimeter_ratio'] = df['tbp_lv_eccentricity'] / (df['tbp_lv_perimeterMM'] + 1e-8)
    df['hue_to_luminance_ratio'] = df['tbp_lv_H'] / (df['tbp_lv_L'] + 1e-8)
    df['size_severity_index'] = df['lesion_size_ratio'] * df['lesion_severity_index']
    df['border_area_interaction'] = df['tbp_lv_norm_border'] * df['tbp_lv_areaMM2']

    # Advanced Geometric Features
    df['elliptical_asymmetry'] = 1 - (df['tbp_lv_minorAxisMM'] / (df['tbp_lv_perimeterMM'] + 1e-8))
    df['compactness'] = df['tbp_lv_areaMM2'] / ((df['tbp_lv_minorAxisMM'] ** 2) + 1e-8)

    # Color and Texture Features
    df['average_chroma'] = (df['tbp_lv_C'] + df['tbp_lv_Cext']) / 2
    df['texture_variation'] = df['tbp_lv_stdL'] - df['tbp_lv_stdLExt']
    df['border_chroma_interaction'] = df['tbp_lv_norm_border'] * df['tbp_lv_C']

    # Spatial and 3D Features
    df['3d_orientation_diff'] = df['lesion_orientation_3d'] - df['lesion_orientation_3d'].mean()
    df['3d_position_weighted_area'] = df['tbp_lv_areaMM2'] * df['position_distance_3d']

    # Composite Indexes
    df['color_shape_severity_index'] = (df['color_contrast_index'] + df['shape_complexity_index'] + df['lesion_severity_index']) / 3
    df['combined_complexity_index'] = (df['tbp_lv_area_perim_ratio'] + df['tbp_lv_norm_border'] + df['tbp_lv_eccentricity'] + df['tbp_lv_radial_color_std_max']) / 4
    df['overall_complexity_score'] = (df['border_complexity'] * 0.5) + (df['color_variance_ratio'] * 0.3) + (df['shape_complexity_index'] * 0.2)

    # Time and Sequence Features (if applicable)
    # Note: Assuming 'time_since_last_lesion' is a feature you would create using a timestamp feature.
    # df['time_since_last_lesion'] = df.groupby('patient_id')['timestamp'].diff().fillna(0)
    df['lesion_evolution_score'] = df.groupby('patient_id')['clin_size_long_diam_mm'].diff().fillna(0)

    # Statistical Features
    df['median_color_difference'] = df[['tbp_lv_deltaA', 'tbp_lv_deltaB', 'tbp_lv_deltaL']].median(axis=1)
    df['range_of_hue'] = abs(df['tbp_lv_H'] - df['tbp_lv_Hext'])
    df['skewness_of_color_distribution'] = df[['tbp_lv_deltaA', 'tbp_lv_deltaB', 'tbp_lv_deltaL']].skew(axis=1)

    # Patient-Level Features
    df['age_standardized_color_variation'] = df['tbp_lv_color_std_mean'] / (df['age_approx'] + 1e-8)
    df['patient_mean_severity'] = df.groupby('patient_id')['lesion_severity_index'].transform('mean')

    # Dimensionality Reduction (Feature Synthesis)
    # Note: PCA or clustering should be performed separately and require more detailed steps.
    # For example:
    # from sklearn.decomposition import PCA
    # pca = PCA(n_components=2)
    # df[['PCA1', 'PCA2']] = pca.fit_transform(df[['tbp_lv_deltaA', 'tbp_lv_deltaB', 'tbp_lv_deltaL']])

    # from sklearn.cluster import KMeans
    # kmeans = KMeans(n_clusters=5)
    # df['cluster_labels'] = kmeans.fit_predict(df[['tbp_lv_x', 'tbp_lv_y', 'tbp_lv_z']])

    # Log and Power Transformations
    df['log_color_contrast_index'] = np.log(df['color_contrast_index'] + 1e-8).fillna(0)
    df['square_root_perimeter'] = np.sqrt(df['tbp_lv_perimeterMM'])

    # Normalize features by patient
    for col in num_cols + new_num_cols:
    # Mean normalization
        df[f'{col}_patient_norm'] = (df[col] - df.groupby('patient_id')[col].transform('mean')) / (df.groupby('patient_id')[col].transform('std').fillna(0) + 1e-8)  # Small epsilon

        df[f'{col}_patient_mean'] = df.groupby('patient_id')[col].transform('mean')

        # Standard deviation by patient_id
        df[f'{col}_patient_std'] = df.groupby('patient_id')[col].transform('std').fillna(0)

        # Median by patient_id
        df[f'{col}_patient_median'] = df.groupby('patient_id')[col].transform('median')

        # Minimum by patient_id
        df[f'{col}_patient_min'] = df.groupby('patient_id')[col].transform('min')

        # Maximum by patient_id
        df[f'{col}_patient_max'] = df.groupby('patient_id')[col].transform('max')

        df[f'{col}_progression'] = df.groupby('patient_id')[col].diff().fillna(0)

    # Count per patient
    df['count_per_patient'] = df.groupby('patient_id')['isic_id'].transform('count')

    # Convert categorical columns to categorical type
    df[cat_cols] = df[cat_cols].astype('category')
    df[cat_cols] = df[cat_cols].fillna(df[cat_cols].mode())

    return df

#data = h5py.File(f'/tmp/ISIC_Competition/isic_hairless_comp_data (1).hdf5', 'r')
train_df = read_data(f'/kaggle/input/isic-2024-challenge/train-metadata.csv')
#old_data = h5py.File(f'/tmp/ISIC_Archive/isic_hairless_archive (1).hdf5', 'r')

if config.debug:
    train_df = train_df[:100]

class SoftLabelEncoder(OneHotEncoder):
    def __init__(self, categories='auto', drop=None, sparse_output=False, dtype=np.float64, handle_unknown='ignore'):
        super().__init__(categories=categories, drop=drop, sparse_output=sparse_output, dtype=dtype, handle_unknown=handle_unknown)

    def transform(self, X):
        # Call the original transform method
        X_transformed = super().transform(X)

        # Apply the soft label encoding: replace non-ones with 0.1
        X_soft = np.where(X_transformed == 1, 1, 0.1)

        return X_soft

def preprocess(train_df):
    global cat_cols

    encoder = OneHotEncoder(sparse_output=False, dtype=np.int32, handle_unknown='ignore')
    encoder.fit(train_df[cat_cols])

    new_cat_cols = [f'onehot_{i}' for i in range(len(encoder.get_feature_names_out()))]

    train_df[new_cat_cols] = encoder.transform(train_df[cat_cols])
    train_df[new_cat_cols] = train_df[new_cat_cols].astype('category')

    for col in cat_cols:
        feature_cols.remove(col)

    feature_cols.extend(new_cat_cols)
    cat_cols = new_cat_cols

    return train_df

train_df = preprocess(train_df)

all_cat_cols = cat_cols
all_num_cols = num_cols + new_num_cols + norm_cols + special_cols

scaler = StandardScaler()
train_df[all_num_cols] = scaler.fit_transform(train_df[all_num_cols])

def get_sample(id_, data_source):
    binary_data = data_source[id_][()]
    image = Image.open(io.BytesIO(binary_data))
    return image

def pAUC(solution, submission, min_tpr: float=0.80) -> float:
    '''
    2024 ISIC Challenge metric: pAUC

    Given a solution file and submission file, this function returns the
    the partial area under the receiver operating characteristic (pAUC)
    above a given true positive rate (TPR) = 0.80.
    https://en.wikipedia.org/wiki/Partial_Area_Under_the_ROC_Curve.

    (c) 2024 Nicholas R Kurtansky, MSKCC

    Args:
        solution: ground truth pd.DataFrame of 1s and 0s
        submission: solution dataframe of predictions of scores ranging [0, 1]

    Returns:
        Float value range [0, max_fpr]
    '''

    # rescale the target. set 0s to 1s and 1s to 0s (since sklearn only has max_fpr)
    v_gt = abs(np.asarray(solution)-1)

    # flip the submissions to their compliments
    v_pred = -1.0*np.asarray(submission)

    max_fpr = abs(1-min_tpr)

    # using sklearn.metric functions: (1) roc_curve and (2) auc
    fpr, tpr, _ = roc_curve(v_gt, v_pred, sample_weight=None)
    if max_fpr is None or max_fpr == 1:
        return auc(fpr, tpr)
    if max_fpr <= 0 or max_fpr > 1:
        raise ValueError("Expected min_tpr in range [0, 1), got: %r" % min_tpr)

    # Add a single point at max_fpr by linear interpolation
    stop = np.searchsorted(fpr, max_fpr, "right")
    x_interp = [fpr[stop - 1], fpr[stop]]
    y_interp = [tpr[stop - 1], tpr[stop]]
    tpr = np.append(tpr[:stop], np.interp(max_fpr, x_interp, y_interp))
    fpr = np.append(fpr[:stop], max_fpr)
    partial_auc = auc(fpr, tpr)

#     # Equivalent code that uses sklearn's roc_auc_score
#     v_gt = abs(np.asarray(solution.values)-1)
#     v_pred = np.array([1.0 - x for x in submission.values])
#     max_fpr = abs(1-min_tpr)
#     partial_auc_scaled = roc_auc_score(v_gt, v_pred, max_fpr=max_fpr)
#     # change scale from [0.5, 1.0] to [0.5 * max_fpr**2, max_fpr]
#     # https://math.stackexchange.com/questions/914823/shift-numbers-into-a-different-range
#     partial_auc = 0.5 * max_fpr**2 + (max_fpr - 0.5 * max_fpr**2) / (1.0 - 0.5) * (partial_auc_scaled - 0.5)

    return(partial_auc)

class Preprocess(nn.Module):
    """Module to perform pre-process using Kornia on torch tensors."""

    @torch.no_grad()  # disable gradients for effiency
    def forward(self, x) -> torch.Tensor:
        x_tmp: np.ndarray = np.array(x)  # HxWxC
        x_tmp = cv2.resize(x_tmp, (384, 384))
        x_out: torch.Tensor = image_to_tensor(x_tmp, keepdim=True)  # CxHxW
        return x_out.float() / 255.0

import numpy as np
from torch.utils.data import Sampler

class CustomBatchSampler(Sampler):
    """Yield a mini-batch of indices.

    Args:
        labels: Labels for building sampling logic.
        batch_size: Size of mini-batch.
    """

    def __init__(self, labels, batch_size):
        self.batch_size = batch_size
        self.labels = labels
        unique, counts = np.unique(self.labels, return_counts=True)
        self.clas_freq = dict(zip(unique, counts))
        self.zeros_indices = np.where(self.labels == 0)[0]
        self.ones_indices = np.where(self.labels == 1)[0]
        self.original_uniform_random = np.arange(start=0, stop=len(self.labels))
        self.uniform_random = self.original_uniform_random.copy()

    def __iter__(self):
        batch = []
        self.uniform_random = self.original_uniform_random.copy()  # Reinitialize uniform_random
        step = 0

        while len(self.uniform_random) > 0:
            chosen_value = np.random.choice([0, 1])
            if chosen_value == 0:
                weighted_index = np.random.choice(self.zeros_indices)
            else:
                weighted_index = np.random.choice(self.ones_indices)

            rand_idx = np.random.choice(self.uniform_random)
            indices = np.where(self.original_uniform_random == rand_idx)[0]
            index = indices[0]

            self.uniform_random = np.delete(self.uniform_random, np.where(self.uniform_random == rand_idx)[0])

            batch.append([rand_idx, weighted_index, step])

            if len(batch) == self.batch_size:
                step += 1
                yield batch
                batch = []

        if len(batch) > 0:
            yield batch

    def __len__(self):
        return len(self.labels) // self.batch_size

def get_rotation_matrix(axis, angle):
    if axis == 'x':
        rotation_matrix = torch.tensor([
            [1, 0, 0],
            [0, math.cos(angle), -math.sin(angle)],
            [0, math.sin(angle), math.cos(angle)]
        ])
    elif axis == 'y':
        rotation_matrix = torch.tensor([
            [math.cos(angle), 0, math.sin(angle)],
            [0, 1, 0],
            [-math.sin(angle), 0, math.cos(angle)]
        ])
    elif axis == 'z':
        rotation_matrix = torch.tensor([
            [math.cos(angle), -math.sin(angle), 0],
            [math.sin(angle), math.cos(angle), 0],
            [0, 0, 1]
        ])
    else:
        raise ValueError("Axis must be 'x', 'y' or 'z'")

    return rotation_matrix

def rotate_rgb_image(image, axis, angle):
    # Get the rotation matrix
    rotation_matrix = get_rotation_matrix(axis, angle)

    # Flatten the spatial dimensions and apply the rotation to each pixel's RGB values
    flattened_image = image.view(3, -1)  # Shape: (3, 384*384)
    rotated_flattened_image = torch.matmul(rotation_matrix, flattened_image)

    # Reshape back to the original image shape
    rotated_image = rotated_flattened_image.view(3, 384, 384)
    return rotated_image

class MixUp(Callable):
    def __init__(self, a=config.beta):
        self.beta = torch.distributions.beta.Beta(torch.tensor([a]), torch.tensor([1]))

    def __call__(self, images, targets, additional_target):
        lambda_ = self.beta.sample()
        new_image = lambda_ * images[0] + (1 - lambda_) * images[1]
        new_target = lambda_ * targets[0] + (1 - lambda_) * targets[1]

        return {
            "inputs" : new_image,
            "target" : new_target,
            "lambda_" : lambda_,
        }

class NoisyMixUp(Callable):
    def __init__(self, a=config.beta):
        self.beta = torch.distributions.beta.Beta(torch.tensor([a]), torch.tensor([1]))

    def __call__(self, images, targets, additional_target):

        noise = torch.normal(mean=0, std=0.025**(1/2), size=(384, 384))
        lambda_1 = self.beta.sample()
        lambda_ = noise + lambda_1
        lambda_ = torch.clamp(lambda_, min=0.0, max=1.0)

        new_image = lambda_ * images[0] + (1 - lambda_) * images[1]
        new_target = lambda_1 * targets[0] + (1 - lambda_1) * targets[1]

        return {
            "inputs" : new_image,
            "target" : new_target,
            "lambda_" : lambda_1,
        }

class RandomElement(Callable):
    def __init__(self, a=config.beta):
        self.beta = torch.distributions.beta.Beta(torch.tensor([a]), torch.tensor([1]))

    def __call__(self, images, targets, additional_target):

        lambda_ = self.beta.sample()
        mask = (torch.rand((3, 384, 384)) < lambda_).float()
        new_image = mask * images[0] + (1 - mask) * images[1]
        new_target = lambda_ * targets[0] + (1 - lambda_) * targets[1]

        return {
            "inputs" : new_image,
            "target" : new_target,
            "lambda_" : lambda_,
        }

class RandomPixel(Callable):
    def __init__(self, a=config.beta):
        self.beta = torch.distributions.beta.Beta(torch.tensor([a]), torch.tensor([1]))

    def __call__(self, images, targets, additional_target):

        lambda_ = self.beta.sample()
        mask = (torch.rand((384, 384)) < lambda_).float()
        new_image = mask * images[0] + (1 - mask) * images[1]
        new_target = lambda_ * targets[0] + (1 - lambda_) * targets[1]

        return {
            "inputs" : new_image,
            "target" : new_target,
            "lambda_" : lambda_,
        }

class BCPlus(nn.Module):
    def __init__(self, a=config.beta):
        self.beta = torch.distributions.beta.Beta(torch.tensor([a]), torch.tensor([1]))
        super().__init__()

    def forward(self, images, targets, additional_target):
        std1, std2 = images[0].std(), images[1].std()
        mean1, mean2 = images[0].mean(), images[1].mean()
        lambda_ = self.beta.sample().item()

        p = 1 / (1 + ((std1/std2) * ((1-lambda_) / lambda_)))

        x_ = (p*(images[0] - mean1) + (1 - p) * (images[1] - mean2)) / (((p ** 2) + (1 - p) ** 2) ** (1/2))
        new_target = lambda_ * targets[0] + (1 - lambda_) * targets[1]


        return {
            "inputs" : x_,
            "target" : torch.tensor(new_target),
            "lambda_" : torch.tensor([lambda_]),
        }

class CutMix(Callable):
    def __init__(self, a=config.beta, W=384, H=384):
        self.W = W
        self.H = H
        self.beta = torch.distributions.beta.Beta(torch.tensor([1.0]), torch.tensor([1.0]))

    def __call__(self, images, targets, additional_target):
        lambda_ = self.beta.sample()
        W = self.W
        H = self.H
        r_x = np.random.randint(0, W)
        r_y = np.random.randint(0, H)
        r_w = int(W * math.sqrt(1 - lambda_))
        r_h = int(H * math.sqrt(1 - lambda_))

        shape = (3, W, H)

        mask = torch.ones(shape)

        mask[:, r_y:r_y+r_h, r_x:r_x+r_w] = 0

        new_image = torch.mul(images[0], mask) + torch.mul(images[1], 1-mask)

        new_target = lambda_ * targets[0] + (1 - lambda_) * targets[1]

        return {
            "inputs" : new_image,
            "target" : new_target,
            "lambda_" : lambda_,
        }

class VHMixUp(Callable):
    def __init__(self, a=config.beta, H=384, W=384):
        self.W = W
        self.H = H
        self.beta = torch.distributions.beta.Beta(torch.tensor([a]), torch.tensor([1]))

    def __call__(self, images, targets, additional_target):
        l1, l2, l3 = [self.beta.sample() for _ in range(3)]

        W = self.W
        H = self.H

        new_image = torch.ones(size=(3, H, W))

        l1_h = int(H * l1)
        l2_w = int(W * l2)

        new_image[:,:l1_h,:l2_w] = images[0][:,:l1_h,:l2_w]
        new_image[:,:l1_h,l2_w:] = l3 * images[0][:,:l1_h,l2_w:] + (1 - l3) * images[1][:,:l1_h,l2_w:]
        new_image[:,l1_h:,:l2_w] = l3 * images[0][:,l1_h:,:l2_w] + (1 - l3) * images[1][:,l1_h:,:l2_w]
        new_image[:,l1_h:,l2_w:] = images[1][:,l1_h:,l2_w:]

        new_target = l3 * targets[0] + (1 - l3) * targets[1]

        return {
            "inputs" : new_image,
            "target" : new_target,
            "lambda_" : l3,
        }

class Random2By2(Callable):
    def __init__(self, p=0.5, H=384, W=384):
        self.beta = torch.distributions.beta.Beta(torch.tensor([1.0]), torch.tensor([1]))
        self.p = p
        self.W = W
        self.H = H

    def __call__(self, images, targets, additional_target):

        H = self.H
        W = self.W

        new_image = torch.ones(size=(3, H, W))

        r_w = np.random.randint(0, int(W * self.p))
        r_h = np.random.randint(0, int(H * self.p))

        quadrant_labels = np.random.choice([0, 1], size=(4))

        new_target = 0
        new_additional_target = 0

        new_image[:,:r_w,:r_h] = images[quadrant_labels[0]][:,:r_w,:r_h]
        new_target += targets[quadrant_labels[0]] * (r_w * r_h)
        new_additional_target += additional_target[quadrant_labels[0]] * (r_w * r_h)
        new_image[:,r_w:,:r_h] = images[quadrant_labels[1]][:,r_w:,:r_h]
        new_target += targets[quadrant_labels[1]] * ((W-r_w) * r_h)
        new_additional_target += additional_target[quadrant_labels[1]] * ((W-r_w) * r_h)
        new_image[:,:r_w,r_h:] = images[quadrant_labels[2]][:,:r_w,r_h:]
        new_target += targets[quadrant_labels[2]] * (r_w * (H-r_h))
        new_additional_target += additional_target[quadrant_labels[0]] * (r_w * (H-r_h))
        new_image[:,r_w:,r_h:] = images[quadrant_labels[3]][:,r_w:,r_h:]
        new_target += targets[quadrant_labels[3]] * ((W-r_w) * (H-r_h))
        new_additional_target += additional_target[quadrant_labels[0]] * ((W-r_w) * (H-r_h))

        new_target /= float((H * W))

        return {
            "inputs" : new_image,
            "target" : torch.tensor(new_target),
            "lambda_" : torch.tensor([new_target]),
        }

class RandomHorizontalConcat(Callable):
    def __init__(self, a=config.beta):
        self.beta = torch.distributions.beta.Beta(torch.tensor([a]), torch.tensor([1]))

    def __call__(self, images, targets, additional_target):

        H = 384
        W = 384

        lambda_ = self.beta.sample()

        new_image = torch.ones(size=(3, H, W))

        r_h = int((lambda_ * H).item())

        new_image[:,:,:r_h] = images[0][:,:,:r_h]
        new_image[:,:,r_h:] = images[1][:,:,r_h:]

        new_target = lambda_ * targets[0] + (1 - lambda_) * targets[1]

        return {
            "inputs" : new_image,
            "target" : new_target,
            "lambda_" : lambda_,
        }

class RandomVerticalConcat(Callable):
    def __init__(self, a=config.beta):
        self.beta = torch.distributions.beta.Beta(torch.tensor([a]), torch.tensor([1]))

    def __call__(self, images, targets, additional_target):

        H = 384
        W = 384

        lambda_ = self.beta.sample()

        new_image = torch.ones(size=(3, H, W))

        r_w = int((lambda_ * W).item())

        new_image[:,:r_w,:] = images[0][:,:r_w,:]
        new_image[:,r_w:,:] = images[1][:,r_w:,:]

        new_target = lambda_ * targets[0] + (1 - lambda_) * targets[1]

        return {
            "inputs" : new_image,
            "target" : new_target,
            "lambda_" : lambda_,
        }

class RandomColumnMix(Callable):
    def __init__(self, a=config.beta):
        self.beta = torch.distributions.beta.Beta(torch.tensor([a]), torch.tensor([1]))

    def __call__(self, images, targets, additional_target):

        H = 384
        W = 384

        lambda_ = self.beta.sample()

        column_mask = torch.bernoulli(torch.full((H,), lambda_.item()))

        # Step 2: Broadcast the mask to match the original image dimensions
        column_mask = column_mask.reshape(1, 1, 384).expand(3, W, -1)

        new_image = column_mask * images[0] + (1 - column_mask) * images[1]

        new_target = lambda_ * targets[0] + (1 - lambda_) * targets[1]

        return {
            "inputs" : new_image,
            "target" : new_target,
            "lambda_" : lambda_,
        }

class RandomRowMix(Callable):
    def __init__(self, a=config.beta):
        self.beta = torch.distributions.beta.Beta(torch.tensor([a]), torch.tensor([1]))

    def __call__(self, images, targets, additional_target):

        H = 384
        W = 384

        lambda_ = self.beta.sample()

        row_mask = torch.bernoulli(torch.full((W,), lambda_.item()))

        # Step 2: Broadcast the mask to match the original image dimensions
        column_mask = row_mask.reshape(1, 384, 1).expand(3, -1, H)

        new_image = row_mask * images[0] + (1 - row_mask) * images[1]

        new_target = lambda_ * targets[0] + (1 - lambda_) * targets[1]

        return {
            "inputs" : new_image,
            "target" : new_target,
            "lambda_" : lambda_,
        }

def tabular_mixup(data, lambda_):
    return data[0] * lambda_ + data[1] * (1 - lambda_)

def tabular_cutmix(data, lambda_):
    mask = (np.array([lambda_ for x in range(len(data[0]))]) > 0.5).astype(np.float32)

    return data[0] * mask + data[1] * (1 - mask)

def exponential_growth(step, min_beta, max_beta, total_steps, steepness=2.0):
    # Ensure that the step is within the valid range
    if step < 0 or step > total_steps:
        raise ValueError("Step must be between 0 and total_steps")

    # Calculate the growth raate and apply the steepness factor
    growth_rate = math.log(max_beta / min_beta) / total_steps
    steep_growth_rate = growth_rate * steepness

    # Calculate the steeper exponential grown value
    beta = min_beta * math.exp(steep_growth_rate * step)

    return beta

class ISICDataset(Dataset):
    def __init__(self, df, bmu=True):
        self.df = df
        self.bmu = bmu
        self.preprocess = Preprocess()
        self.fp_hdf = h5py.File("/kaggle/input/isic-2024-challenge/train-image.hdf5", mode="r")
        self.targets = df["target"].values
        self.mixups = [
            MixUp(),
            CutMix(),
            RandomVerticalConcat(),
            RandomHorizontalConcat(),
            VHMixUp(),
            RandomPixel(),
            NoisyMixUp(),
            RandomElement(),
            RandomRowMix(),
            RandomColumnMix(),
            BCPlus(),
            Random2By2(),
        ]

        self.tab_aug = [
            tabular_mixup,
        ]
        self.onehotencode = {
            0 : np.array([1, 0]),
            1 : np.array([0, 1]),
        }
        self.total_steps = (len(self.df) // config.train_batch_size) * config.epochs

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idxs):

        if self.bmu:
            step = idxs[-1]
            idxs = idxs[:-1]
            images = []
            targets = []
            additional_targets = []
            row_features = []

            for idx in idxs:
                row = self.df.iloc[idx]
                sample_id = row["isic_id"]

                a_target = float(row["tbp_lv_nevi_confidence"])

                additional_targets.append(a_target)

                row_features.append(row[feature_cols])

                if row["year"] == 2024:
                    image = Image.open(BytesIO(self.fp_hdf[sample_id][()]))
                elif row["year"] == 0:
                    image = Image.open(row["image_pth"])
                else:
                    image = Image.open(f"/kaggle/input/images-resized-224x224/archive_data/{sample_id}.jpg")

                images.append(self.preprocess(image))
                targets.append(row["target"])

            if (targets[0] == targets[1]):
                contrastive_target = 1
            else:
                contrastive_target = -1

            aug = random.choice(self.mixups)
            tab_aug = random.choice(self.tab_aug)

            outputs = aug(images, targets, additional_targets)

            outputs["target"] = outputs["target"].reshape((1,))

            outputs["tabular_features"] = row_features

            if config.pretrain or config.manifold_mixup:
                outputs["inputs_1"] = images[0]
                outputs["inputs_2"] = images[1]
                outputs["contrastive_target"] = contrastive_target

        #    new_beta_value = exponential_growth(step, 0.3, 0.4, self.total_steps)

        #    if step % config.train_batch_size == 0:
        #        for aug in self.mixups:
        #            aug.beta = self.beta = torch.distributions.beta.Beta(torch.tensor([new_beta_value]), torch.tensor([1]))

        #    outputs["beta"] = torch.tensor([new_beta_value])

            return outputs

        else:
            row = self.df.iloc[idxs]
            sample_id = row["isic_id"]

            if row["year"] == 2024:
                image = Image.open(BytesIO(self.fp_hdf[sample_id][()]))
            elif row["year"] == 0:
                image = Image.open(row["image_pth"])
            else:
                image = Image.open(f"/kaggle/input/images-resized-224x224/archive_data/{sample_id}.jpg")

            inputs = {
                "inputs" : self.preprocess(image),
            }

            inputs["target"] = np.array([row["target"]])
            inputs["tabular_features"] = row[feature_cols].to_numpy(dtype=np.float64)

            return inputs

    def get_labels(self):
        return self.df["target"].values

    def plot_aug(self):
        images = [Preprocess()(Image.open(pth)) for pth in ["/content/cat.jpg", "/content/dog.jpg"]]
        targets = [np.array([1, 0]), np.array([0, 1])]
        for aug in self.mixups:
            x = aug(images, targets, targets)
            plt.imshow(x["inputs"].detach().cpu().numpy().reshape(384, 384, 3))
            plt.show()
            print(x["target"])

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    return {
        "pAUC" : pAUC(labels, predictions),
    }

id_list = train_df.patient_id.value_counts().index
FOLDS = 5; CT = len(id_list)//FOLDS
s = np.zeros((FOLDS)); t = np.zeros((FOLDS)); i = 0
for k in range(CT+1):
    if k!=CT:
        for j in range(FOLDS):
            s[j] = train_df.loc[train_df.patient_id==id_list[i+j],'target'].sum()
        xx = np.argsort(s); yy = np.argsort(-t)
        t[yy] = t[yy] + s[xx]
        for j in range(FOLDS):
            train_df.loc[train_df.patient_id==id_list[i+xx[j]],'fold'] = yy[j]
        i += FOLDS
    else:
        for j in range(len(id_list)-CT*FOLDS):
            train_df.loc[train_df.patient_id==id_list[i+j],'fold'] = j

train_df["year"] = 2024

class RGB2HSV(nn.Module):
    def __init__(self):
        super(RGB2HSV, self).__init__()

    def forward(self, rgb: torch.Tensor) -> torch.Tensor:
        cmax, cmax_idx = torch.max(rgb, dim=1, keepdim=True)
        cmin = torch.min(rgb, dim=1, keepdim=True)[0]
        delta = cmax - cmin
        hsv_h = torch.empty_like(rgb[:, 0:1, :, :])

        # Handle the cases where delta is 0 to avoid division by zero
        cmax_idx[delta == 0] = 3
        hsv_h[cmax_idx == 0] = (((rgb[:, 1:2] - rgb[:, 2:3]) / delta) % 6)[cmax_idx == 0]
        hsv_h[cmax_idx == 1] = (((rgb[:, 2:3] - rgb[:, 0:1]) / delta) + 2)[cmax_idx == 1]
        hsv_h[cmax_idx == 2] = (((rgb[:, 0:1] - rgb[:, 1:2]) / delta) + 4)[cmax_idx == 2]
        hsv_h[cmax_idx == 3] = 0.
        hsv_h /= 6.

        hsv_s = torch.where(cmax == 0, torch.tensor(0.).type_as(rgb), delta / cmax)
        hsv_v = cmax

        return torch.cat([rgb, hsv_h, hsv_s, hsv_v], dim=1)

class HairRemovalTransform(Callable):
    def __init__(self, kernel_size=(4, 4), threshold=4, inpaint_radius=2):
        self.kernel_size = kernel_size
        self.threshold = threshold
        self.inpaint_radius = inpaint_radius

    def top_hat_transform(self, input_image):
        input_image_gray = input_image.convert('L')
        filterSize = self.kernel_size
        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, filterSize)
        tophat_img = cv2.morphologyEx(np.array(input_image_gray), cv2.MORPH_BLACKHAT, kernel)
        return Image.fromarray(tophat_img)

    def apply_threshold(self, arr):
        arr_thresholded = np.where(np.array(arr) > self.threshold, 255, 0).astype(np.uint8)
        return Image.fromarray(arr_thresholded)

    def __call__(self, img):
        transformed_img = self.top_hat_transform(img)
        mask = self.apply_threshold(transformed_img)
        new_img = cv2.inpaint(np.array(img), np.array(mask), self.inpaint_radius, cv2.INPAINT_NS)
        return Image.fromarray(new_img)

class HairRemovalTransformKornia(nn.Module):
    def __init__(self, kernel_size=(4, 4), threshold=4, inpaint_radius=2):
        super(HairRemovalTransformKornia, self).__init__()
        self.kernel_size = kernel_size
        self.threshold = threshold
        self.inpaint_radius = inpaint_radius
        self.kernel = self.create_structuring_element(kernel_size)
        self.rgb_to_grayscale = kc.RgbToGrayscale()

    def create_structuring_element(self, kernel_size):
        return torch.ones(kernel_size, dtype=torch.float32)

    def top_hat_transform(self, input_tensor):
        input_gray = self.rgb_to_grayscale(input_tensor)
        eroded = km.erosion(input_gray, self.kernel.to(input_tensor.device))
        dilated = km.dilation(eroded, self.kernel.to(input_tensor.device))
        tophat_img = input_gray - dilated
        return tophat_img

    def apply_threshold(self, tensor):
        thresholded_tensor = (tensor > self.threshold / 255.0).float()
        return thresholded_tensor

    def inpaint_batch(self, imgs, masks):
        inpainted_imgs = []
        for img, mask in zip(imgs, masks):
            img_np = img.permute(1, 2, 0).cpu().numpy() * 255
            mask_np = mask.squeeze().cpu().numpy() * 255
            inpainted_img_np = cv2.inpaint(img_np.astype(np.uint8), mask_np.astype(np.uint8), self.inpaint_radius, cv2.INPAINT_NS)
            inpainted_img = torch.tensor(inpainted_img_np).permute(2, 0, 1).float() / 255.0
            inpainted_imgs.append(inpainted_img.to(img.device))
        return torch.stack(inpainted_imgs)

    def forward(self, img_batch):
        # Assuming img_batch is a tensor of shape (B, C, H, W)
        tophat_img = self.top_hat_transform(img_batch)
        mask = self.apply_threshold(tophat_img)
        new_img_batch = self.inpaint_batch(img_batch, mask)
        return new_img_batch

class DataAugmentation(nn.Module):
    """Module to perform data augmentation using Kornia on torch tensors."""

    def __init__(self, transforms) -> None:
        super().__init__()
        self.transforms = transforms

    @torch.no_grad()
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x_out = self.transforms(x)
        return x_out

class RandomTranspose(nn.Module):
    def __init__(self, p=0.5):
        super(RandomTranspose, self).__init__()
        self.p = p

    def forward(self, x):
        if random.random() < self.p:
            return torch.transpose(x, 2, 3)
        return x

class RandomChoice(nn.Module):
    def __init__(self, transforms, p=0.5):
        super(RandomChoice, self).__init__()
        self.transforms = transforms
        self.p = p

    def forward(self, x):
        if random.random() < self.p:
            transform = random.choice(self.transforms)
            return transform(x)
        return x

class MixUpOrCutMix(nn.Module):
    def __init__(self):
        super(MixUpOrCutMix, self).__init__()
        self.transforms = [
            K.RandomCutMixV2(beta=1.0, data_keys=["input", "class"]),
            K.RandomMixUpV2(lambda_val=(0, 0.2), data_keys=["input", "class"]),
        ]

    @torch.no_grad()
    def forward(self, inputs, target):
        transform = random.choice(self.transforms)
        inputs, label_set = transform(inputs, target)
        return inputs, label_set.squeeze()[:,1]

import os.path
def file_exists(pth):
    return os.path.isfile(pth)

class GeM(nn.Module):
    def __init__(self, p=3, eps=1e-6):
        super(GeM, self).__init__()
        self.p = nn.Parameter(torch.ones(1)*p)
        self.eps = eps

    def forward(self, x):
        return self.gem(x, p=self.p, eps=self.eps)

    def gem(self, x, p=3, eps=1e-6):
        return t_func.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)

    def __repr__(self):
        return self.__class__.__name__ + \
                '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + \
                ', ' + 'eps=' + str(self.eps) + ')'

class AttentionLayer(nn.Module):
    def __init__(self, input_dim):
        super(AttentionLayer, self).__init__()
        self.batch_norm = nn.BatchNorm2d(num_features=input_dim)  # Assuming pt_features has shape [batch, channels, height, width]
        self.dropout = nn.Dropout(p=0.5)
        self.conv1 = nn.Conv2d(in_channels=input_dim, out_channels=64, kernel_size=1, padding=0)
        self.conv2 = nn.Conv2d(in_channels=64, out_channels=16, kernel_size=1, padding=0)
        self.conv3 = nn.Conv2d(in_channels=16, out_channels=8, kernel_size=1, padding=0)
        self.conv4 = nn.Conv2d(in_channels=8, out_channels=1, kernel_size=1, padding=0)
        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()
        self.glo_pool = nn.AdaptiveAvgPool2d((1, 1))

    def forward(self, x):
        x1 = self.batch_norm(x)
        x = self.dropout(x1)
        x = self.relu(self.conv1(x))
        x = self.relu(self.conv2(x))
        x = self.relu(self.conv3(x))
        x = self.conv4(x)
        x = self.sigmoid(x)
        x = torch.multiply(x1, x)
        x = self.glo_pool(x)
        return x

class Normalize(nn.Module):
    def __init__(self, mean: torch.Tensor, std: torch.Tensor):
        super().__init__()
        self.mean = mean
        self.std = std

    def __call__(self, tensor: torch.Tensor) -> torch.Tensor:
        """
        Normalize a tensor image with mean and standard deviation.

        Args:
            tensor (torch.Tensor): Input tensor image of shape (C, H, W) or (B, C, H, W).

        Returns:
            torch.Tensor: Normalized tensor image.
        """
        if tensor.dim() < 3:
            raise ValueError(f"Expected tensor to have at least 3 dimensions, got {tensor.dim()}.")

        # Calculate mean and std for each channel
        mean = self.mean.to(tensor.device).view(-1, 1, 1)
        std = self.std.to(tensor.device).view(-1, 1, 1)

        # Normalize tensor image
        tensor = (tensor - mean) / std

        return tensor

class Microscope(nn.Module):
    def __init__(self, p: float = 0.5):
        super(Microscope, self).__init__()
        self.p = p

    def forward(self, img: torch.Tensor) -> torch.Tensor:
        if random.random() < self.p:
            b, c, h, w = img.size()
            mask = torch.ones((b, h, w), device=img.device, dtype=img.dtype)
            for i in range(b):
                center = (h // 2, w // 2)
                radius = random.randint(h // 2 - 3, h // 2 + 15)
                y, x = torch.meshgrid(torch.arange(h, device=img.device), torch.arange(w, device=img.device))
                dist = torch.sqrt((x - center[1]) ** 2 + (y - center[0]) ** 2)
                mask[i] = torch.where(dist >= radius, torch.tensor(0.0, device=img.device), mask[i])
            img = img * mask.unsqueeze(1)
        return img

class CoarseDropout(nn.Module):
    def __init__(self, p=0.5, size=(4, 4), holes=20):
        super(CoarseDropout, self).__init__()
        self.p = p
        self.size = size
        self.holes = holes

    def forward(self, image):
        if torch.rand(1).item() > self.p:
            return image

        N, C, H, W = image.shape
        dropout_mask = torch.ones((N, C, H, W), device=image.device)

        for _ in range(self.holes):
            top = np.random.randint(0, H - self.size[0] + 1)
            left = np.random.randint(0, W - self.size[1] + 1)

            dropout_mask[:, :, top:top + self.size[0], left:left + self.size[1]] = 0

        return image * dropout_mask

class FocalLoss(nn.Module):
    def __init__(self, alpha=1, gamma=2, reduction='mean'):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, inputs, targets):
        BCE_loss = t_func.binary_cross_entropy_with_logits(inputs, targets, reduction='none')
        pt = torch.exp(-BCE_loss)  # prevents nans when probability 0
        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss

        if self.reduction == 'mean':
            return torch.mean(F_loss)
        elif self.reduction == 'sum':
            return torch.sum(F_loss)
        else:
            return F_loss

class SupervisedContrastiveLoss(nn.Module):
    def __init__(self, temperature=0.1):
        super(SupervisedContrastiveLoss, self).__init__()
        self.temperature = temperature

    def forward(self, feature_vectors, labels):
        # Normalize feature vectors
        feature_vectors_normalized = t_func.normalize(feature_vectors, p=2, dim=1)
        # Compute logits
        logits = torch.div(
            torch.matmul(
                feature_vectors_normalized, torch.transpose(feature_vectors_normalized, 0, 1)
            ),
            self.temperature,
        )
        return losses.NTXentLoss(temperature=0.07)(logits, torch.squeeze(labels))

class SupConLoss(nn.Module):
    """Supervised Contrastive Learning: https://arxiv.org/pdf/2004.11362.pdf.
    It also supports the unsupervised contrastive loss in SimCLR"""
    def __init__(self, temperature=0.07, contrast_mode='all',
                 base_temperature=0.07):
        super(SupConLoss, self).__init__()
        self.temperature = temperature
        self.contrast_mode = contrast_mode
        self.base_temperature = base_temperature

    def forward(self, features, labels=None, mask=None):
        """Compute loss for model. If both `labels` and `mask` are None,
        it degenerates to SimCLR unsupervised loss:
        https://arxiv.org/pdf/2002.05709.pdf

        Args:
            features: hidden vector of shape [bsz, n_views, ...].
            labels: ground truth of shape [bsz].
            mask: contrastive mask of shape [bsz, bsz], mask_{i,j}=1 if sample j
                has the same class as sample i. Can be asymmetric.
        Returns:
            A loss scalar.
        """
        device = (torch.device('cuda')
                  if features.is_cuda
                  else torch.device('cpu'))

        if len(features.shape) < 3:
            raise ValueError('`features` needs to be [bsz, n_views, ...],'
                             'at least 3 dimensions are required')
        if len(features.shape) > 3:
            features = features.view(features.shape[0], features.shape[1], -1)

        batch_size = features.shape[0]
        if labels is not None and mask is not None:
            raise ValueError('Cannot define both `labels` and `mask`')
        elif labels is None and mask is None:
            mask = torch.eye(batch_size, dtype=torch.float32).to(device)
        elif labels is not None:
            labels = labels.contiguous().view(-1, 1)
            if labels.shape[0] != batch_size:
                raise ValueError('Num of labels does not match num of features')
            mask = torch.eq(labels, labels.T).float().to(device)
        else:
            mask = mask.float().to(device)

        contrast_count = features.shape[1]
        contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)
        if self.contrast_mode == 'one':
            anchor_feature = features[:, 0]
            anchor_count = 1
        elif self.contrast_mode == 'all':
            anchor_feature = contrast_feature
            anchor_count = contrast_count
        else:
            raise ValueError('Unknown mode: {}'.format(self.contrast_mode))

        # compute logits
        anchor_dot_contrast = torch.div(
            torch.matmul(anchor_feature, contrast_feature.T),
            self.temperature)
        # for numerical stability
        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)
        logits = anchor_dot_contrast - logits_max.detach()

        # tile mask
        mask = mask.repeat(anchor_count, contrast_count)
        # mask-out self-contrast cases
        logits_mask = torch.scatter(
            torch.ones_like(mask),
            1,
            torch.arange(batch_size * anchor_count).view(-1, 1).to(device),
            0
        )
        mask = mask * logits_mask

        # compute log_prob
        exp_logits = torch.exp(logits) * logits_mask
        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))

        # compute mean of log-likelihood over positive
        # modified to handle edge cases when there is no positive pair
        # for an anchor point.
        # Edge case e.g.:-
        # features of shape: [4,1,...]
        # labels:            [0,1,1,2]
        # loss before mean:  [nan, ..., ..., nan]
        mask_pos_pairs = mask.sum(1)
        mask_pos_pairs = torch.where(mask_pos_pairs < 1e-6, 1, mask_pos_pairs)
        mean_log_prob_pos = (mask * log_prob).sum(1) / mask_pos_pairs

        # loss
        loss = - (self.temperature / self.base_temperature) * mean_log_prob_pos
        loss = loss.view(anchor_count, batch_size).mean()

        return loss

class BinarySupConLoss(nn.Module):
    def __init__(self, temperature=0.07, contrast_mode='all', base_temperature=0.07):
        super(BinarySupConLoss, self).__init__()
        self.temperature = temperature
        self.contrast_mode = contrast_mode
        self.base_temperature = base_temperature

    def forward(self, features, labels):
        """Compute binary contrastive loss.

        Args:
            features: hidden vector of shape [bsz, ...].
            labels: ground truth of shape [bsz].
        Returns:
            A loss scalar.
        """
        device = (torch.device('cuda') if features.is_cuda else torch.device('cpu'))

        batch_size = features.shape[0]

        if labels is None:
            raise ValueError('Labels must be provided for binary classification.')

        # Generate contrastive mask
        labels = labels.contiguous().view(-1, 1)
        if labels.shape[0] != batch_size:
            raise ValueError('Number of labels does not match number of features')
        mask = torch.eq(labels, labels.T).float().to(device)

        contrast_count = 1  # Binary classification, hence one contrast per example
        anchor_feature = features
        anchor_count = contrast_count

        # compute logits
        anchor_dot_contrast = torch.div(
            torch.matmul(anchor_feature, anchor_feature.T),
            self.temperature
        )
        # for numerical stability
        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)
        logits = anchor_dot_contrast - logits_max.detach()

        # tile mask
        mask = mask.repeat(anchor_count, contrast_count)
        # mask-out self-contrast cases
        logits_mask = torch.scatter(
            torch.ones_like(mask),
            1,
            torch.arange(batch_size * anchor_count).view(-1, 1).to(device),
            0
        )
        mask = mask * logits_mask

        # compute log_prob
        exp_logits = torch.exp(logits) * logits_mask
        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))

        # compute mean of log-likelihood over positive
        mask_pos_pairs = mask.sum(1)
        mask_pos_pairs = torch.where(mask_pos_pairs < 1e-6, 1, mask_pos_pairs)
        mean_log_prob_pos = (mask * log_prob).sum(1) / mask_pos_pairs

        # loss
        loss = - (self.temperature / self.base_temperature) * mean_log_prob_pos
        loss = loss.mean()

        return loss

class AUCMixupLoss(nn.Module):
    def __init__(self, m=1.0, a=0.1):
        super().__init__()
        self.a = a
        self.m = m

    def forward(self, logits, labels):
        proba = nn.Sigmoid()(logits)

        batch_size = len(proba)

        l_sum = labels.sum() + 1e-6
        l1_sum = (1 - labels).sum() + 1e-6

        a = torch.matmul(proba, labels) / l_sum
        b = torch.matmul(proba, 1 - labels) / l1_sum

        loss = torch.pow(torch.matmul(proba - a, labels), 2) / l_sum
        loss += torch.pow(torch.matmul(proba - b, 1 - labels), 2) / l1_sum
        loss += 2 * self.a * (self.m - (torch.matmul(proba, labels) / l_sum) + (torch.matmul(proba, 1 - labels) / l1_sum))
        loss -= self.a ** 2

        return loss

def cross_entropy(preds, targets, reduction='none'):
    log_softmax = nn.LogSoftmax(dim=-1)
    loss = (-targets * log_softmax(preds)).sum(1)
    if reduction == "none":
        return loss
    elif reduction == "mean":
        return loss.mean()

class MultiheadCrossAttention(nn.Module):
    def __init__(self, hidden_dim):
        super(MultiheadCrossAttention, self).__init__()
        self.query = nn.Linear(hidden_dim, hidden_dim)
        self.key = nn.Linear(hidden_dim, hidden_dim)
        self.value = nn.Linear(hidden_dim, hidden_dim)
        self.hidden_dim = hidden_dim
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, x):
        Q = self.query(x[0])  # shape: (batch_size, hidden_dim)
        K = self.key(x[1])    # shape: (batch_size, hidden_dim)
        V = self.value(x[2])  # shape: (batch_size, hidden_dim)

        # Compute attention scores
        attention_scores = torch.matmul(Q, K.T) / (self.hidden_dim ** 0.5)  # shape: (batch_size, batch_size)
        attention_weights = self.softmax(attention_scores)  # shape: (batch_size, batch_size)

        # Apply the attention weights to the values
        attention_output = torch.matmul(attention_weights, V)  # shape: (batch_size, hidden_dim)
        return attention_output

class IntersampleAttention(nn.Module):
    def __init__(self, embed_dim, num_features, num_heads, dropout=0.2):
        super().__init__()
        self.attention = nn.MultiheadAttention(embed_dim*num_features, num_heads, dropout=dropout)

    def forward(self, x):
        b, n, d = x.shape
        x = x.reshape(1, b, n*d)
        x = self.attention(x, x, x)[0].reshape(b, n, d)
        return x

class SaintBlock(nn.Module):
    def __init__(self, embed_dim, num_features, num_heads, ff_hidden_dim, dropout=0.1):
        super(SaintBlock, self).__init__()

        # Multi-head Self-Attention
        self.attention = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)

        self.intersample_attention = IntersampleAttention(embed_dim, num_features, num_heads, dropout=dropout)

        # Feed-Forward Network
        self.ffn = nn.Sequential(
            nn.Linear(embed_dim, ff_hidden_dim),
            nn.ReLU(),
            nn.Linear(ff_hidden_dim, embed_dim)
        )

        # Layer Norms
        self.norm1 = nn.LayerNorm(embed_dim)
        self.norm2 = nn.LayerNorm(embed_dim)

        # Dropout
        self.dropout = nn.Dropout(dropout)

        self.ffn_2 = nn.Sequential(
            nn.Linear(embed_dim, ff_hidden_dim),
            nn.ReLU(),
            nn.Linear(ff_hidden_dim, embed_dim)
        )

        # Layer Norms
        self.norm1_2 = nn.LayerNorm(embed_dim)
        self.norm2_2 = nn.LayerNorm(embed_dim)

        # Dropout
        self.dropout_2 = nn.Dropout(dropout)

    def forward(self, x):
        # Self-Attention
        attn_output, _ = self.attention(x, x, x)
        x = self.norm1(x + self.dropout(attn_output))

        # Feed-Forward Network
        ffn_output = self.ffn(x)
        x = self.norm2(x + self.dropout(ffn_output))

        attn_output = self.intersample_attention(x)
        x = self.norm1_2(x + self.dropout(attn_output))

        # Feed-Forward Network
        ffn_output = self.ffn_2(x)
        x = self.norm2_2(x + self.dropout_2(ffn_output))

        return x

class TabularEncoder(nn.Module):
    def __init__(self, num_features, hidden_size, pooler="cls", n_layers=1):
        super().__init__()
        self.num_features = num_features
        self.linear_stack = nn.ModuleList([nn.Linear(1, hidden_size) for i in range(num_features)])
        self.transformer_encoder_stack = nn.ModuleList([SaintBlock(hidden_size, num_features, hidden_size, 4) for i in range(n_layers)])
        self.pooler = pooler

    def get_embeddings(self, x):
        x = x.unsqueeze(-1)

        embeddings = []

        for i in range(self.num_features):
            embeddings.append(self.linear_stack[i](x[:,i,:].unsqueeze(1)))

        embeddings = torch.cat(embeddings, dim=1)

        return embeddings

    def forward(self, x):
        x = self.get_embeddings(x)

        for transformer_encoder in self.transformer_encoder_stack:
            x = transformer_encoder(x)

        if self.pooler == "cls":
            return x[:,0,:]

class ISICModel(nn.Module):
    def __init__(self, model_name, pretrained=False):
        super(ISICModel, self).__init__()

        self.model = timm.create_model(model_name=model_name, pretrained=True, in_chans=3,  num_classes=0, global_pool='')
        self.fc=nn.Linear(self.model.head_hidden_size, 256)
        self.fc_final = nn.Linear(self.model.head_hidden_size, 1)

        self.tabular_fc = nn.Sequential(
            nn.Linear(len(feature_cols), 8192),
            nn.SELU(),
            nn.BatchNorm1d(8192),
            nn.Dropout(0.2),
            nn.Linear(8192, 4096),
            nn.SELU(),
            nn.BatchNorm1d(4096),
            nn.Dropout(0.2),
            nn.Linear(4096, 2048),
            nn.SELU(),
            nn.BatchNorm1d(2048),
            nn.Dropout(0.2),
            nn.Linear(2048, 1024),
            nn.SELU(),
            nn.BatchNorm1d(1024),
            nn.Dropout(0.2),
            nn.Linear(1024, 512),
            nn.SELU(),
            nn.BatchNorm1d(512),
            nn.Dropout(0.2),
            nn.Linear(512, 256),
            nn.SELU(),
            nn.BatchNorm1d(256),
        )

        self.dropout = nn.ModuleList([
            nn.Dropout(0.5) for i in range(5)
        ])

        mean = torch.tensor([0.485, 0.456, 0.406])
        std = torch.tensor([0.229, 0.224, 0.225])

        self.pooler = AttentionLayer(self.model.head_hidden_size)

        self.proj_layer = nn.Linear(self.model.head_hidden_size, config.projection_dim)

        self.tab_attn = MultiheadCrossAttention(256)
        self.cv_attn = MultiheadCrossAttention(256)
        self.tab_self_attn = MultiheadCrossAttention(256)
        self.cv_self_attn = MultiheadCrossAttention(256)

        self.initialize_weights(self.tab_attn)
        self.initialize_weights(self.cv_attn)
        self.initialize_weights(self.tab_self_attn)
        self.initialize_weights(self.cv_self_attn)

        self.train_transform = DataAugmentation(
            nn.Sequential(
            #    K.RandomResizedCrop(config.size),
        #        HairRemovalTransformKornia(),
                RandomTranspose(p=0.5),
                K.RandomVerticalFlip(p=0.5),
                K.RandomHorizontalFlip(p=0.5),
                K.RandomBrightness(brightness=(0.8, 1.2), p=0.75),
                K.RandomContrast(contrast=(0.8, 1.2), p=0.75),
                K.RandomSharpness(sharpness=0.7, p=0.75),
                RandomChoice([
                    K.RandomMotionBlur(kernel_size=5, angle=(0., 360.), direction=(0., 1.), p=0.5),
                    K.RandomMedianBlur(kernel_size=5, p=0.5),
                    K.RandomGaussianBlur(kernel_size=5, sigma=(0.1, 2.0), p=0.5),
                ], p=0.7),
                RandomChoice([
                    K.RandomPerspective(distortion_scale=0.3, p=0.5),
                    K.RandomElasticTransform(alpha=(3.0, 3.0), sigma=(50.0, 50.0), p=0.5),
                ], p=0.7),
                K.ColorJitter(hue=0.1, saturation=0.2, brightness=0.1, p=0.5),
                K.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=15, resample='BILINEAR', padding_mode='zeros', p=0.85),
                K.RandomErasing(scale=(0.02, 0.02), ratio=(0.3, 0.3), p=0.7),
                CoarseDropout(p=0.5, size=(3, 3), holes=5),
                Normalize(mean, std),
            )
        )

        self.valid_transform = DataAugmentation(
            nn.Sequential(
        #        HairRemovalTransformKornia(),
              #  K.CenterCrop(config.size),
                Normalize(mean, std),
            )
        )

        self.temperature = config.temperature
        self.tab_drp = nn.Dropout(0.2)
        self.cv_drp = nn.Dropout(0.2)
        self.step = 0


    def forward(self, inputs, target, tabular_features, beta=None, inputs_1=None, inputs_2=None, contrastive_target=None, return_embeddings=False, additional_target=None, lambda_=None):
    #    if self.training:
    #        self.step += 1
    #        wandb.log({"step": self.step, "beta": beta.mean().item()})

        if self.training:
            inputs = self.train_transform(inputs)
            if config.pretrain:
                inputs_1 = self.train_transform(inputs_1)
                inputs_2 = self.train_transform(inputs_2)
        else:
            inputs = self.valid_transform(inputs)

        x = self.model(inputs)
        pool = self.pooler(x).reshape(len(inputs),-1)

        if not self.training:
            tab_emb_initial = self.tabular_fc(tabular_features.float())
        else:
            tab_features1 = tabular_features[:,0,:].float()
            tab_features2 = tabular_features[:,1,:].float()

            tab_emb_initial1 = self.tabular_fc(tab_features1)
            tab_emb_initial2 = self.tabular_fc(tab_features2)
            tab_emb_initial = tab_emb_initial1 * lambda_ + (1 - lambda_) * tab_emb_initial2

        tab_emb_initial = tab_emb_initial.float()
        cv_emb_initial = self.fc(pool)

        tab_cross_emb = self.tab_attn([cv_emb_initial, tab_emb_initial, tab_emb_initial]) + tab_emb_initial
        cv_cross_emb = self.cv_attn([tab_emb_initial, cv_emb_initial, cv_emb_initial]) + cv_emb_initial

        tab_self_emb = self.tab_self_attn([tab_emb_initial, tab_emb_initial, tab_emb_initial]) + tab_emb_initial
        cv_self_emb = self.cv_self_attn([cv_emb_initial, cv_emb_initial, cv_emb_initial]) + cv_emb_initial

        tab_cross_emb = self.tab_drp(tab_cross_emb)
        cv_cross_emb = self.cv_drp(cv_cross_emb)
        tab_self_emb = self.tab_drp(tab_self_emb)
        cv_self_emb = self.cv_drp(cv_self_emb)

    #    clip_loss = BinarySupConCLIPLoss(0.3)(tab_emb, cv_emb, target)

      #  pool = torch.cat((tab_cross_emb, cv_cross_emb, tab_self_emb, cv_self_emb), dim=-1)

        if self.training:
            logit=0

            for i in range(len(self.dropout)):
                logit += self.fc_final(self.dropout[i](pool))/len(self.dropout)

            if config.pretrain:
                x1 = self.model(inputs_1)
                x2 = self.model(inputs_2)

                pool1 = self.pooler(x1).reshape(len(inputs),-1)
                pool2 = self.pooler(x2).reshape(len(inputs),-1)

                pool1 = self.proj_layer(pool1)
                pool2 = self.proj_layer(pool2)

                pool1 = t_func.normalize(pool1, dim=-1)
                pool2 = t_func.normalize(pool2, dim=-1)

                loss = nn.BCEWithLogitsLoss()(logit.float(), target.float().squeeze()) * (config.loss_weights[0]) + (nn.CosineEmbeddingLoss()(pool1, pool2, contrastive_target)) * config.loss_weights[0] + nn.BCEWithLogitsLoss()(logit2, additional_target)
            else:
                if False:
                    x1 = self.model(inputs_1)
                    x2 = self.model(inputs_2)

                    pool1 = self.pooler(x1).reshape(len(inputs),-1)
                    pool2 = self.pooler(x2).reshape(len(inputs),-1)

                    pool_final = pool2 * (1 - lambda_) + pool1 * lambda_

                    logit_manifold = self.fc(pool_final)

                loss = nn.BCEWithLogitsLoss()(logit.float().squeeze(), target.float().squeeze()) #+ nn.BCEWithLogitsLoss()(logit_manifold.float().squeeze(), target.float().squeeze())
        else:
            logit = self.fc_final(pool)
            loss = nn.BCEWithLogitsLoss()(logit.float().squeeze(), target.float().squeeze())

        if return_embeddings:
            pool = t_func.adaptive_avg_pool2d(x,1).reshape(len(inputs),-1)
            pool = self.proj_layer(pool)
            return t_func.normalize(pool, dim=-1)
        else:
            return SequenceClassifierOutput(loss=loss, logits=nn.Sigmoid()(logit))

    def initialize_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.xavier_uniform_(module.weight)
            if module.bias is not None:
                nn.init.zeros_(module.bias)

from torch.optim.lr_scheduler import _LRScheduler

class WarmupCosineAnnealingWarmRestarts(_LRScheduler):
    def __init__(self, optimizer, warmup_steps, T_0, T_mult=1, eta_min=0, last_epoch=-1):
        self.warmup_steps = warmup_steps
        self.T_0 = T_0
        self.T_mult = T_mult
        self.eta_min = eta_min
        self.T_i = T_0
        self.cycle = 0
        self.base_lrs = [group['lr'] for group in optimizer.param_groups]
        super(WarmupCosineAnnealingWarmRestarts, self).__init__(optimizer, last_epoch)

    def get_lr(self):
        if self.last_epoch < self.warmup_steps:
            return [(self.eta_min + (base_lr - self.eta_min) * self.last_epoch / self.warmup_steps)
                    for base_lr in self.base_lrs]

        if self.last_epoch == self.warmup_steps:
            self.cycle = 0
            self.T_i = self.T_0

        epoch_in_cycle = self.last_epoch - self.warmup_steps - sum([self.T_0 * (self.T_mult ** i) for i in range(self.cycle)])

        if epoch_in_cycle >= self.T_i:
            self.cycle += 1
            epoch_in_cycle = 0
            self.T_i *= self.T_mult

        cosine_decay = 0.5 * (1 + math.cos(math.pi * epoch_in_cycle / self.T_i))
        return [self.eta_min + (base_lr - self.eta_min) * cosine_decay for base_lr in self.base_lrs]

def get_differential_parameters(model, min_lr, max_lr, weight_decay=0.01):
    return [
        {
            "params": [p for n, p in model.named_parameters() if "model" in n],
            "lr": min_lr,
            "weight_decay": weight_decay,
        },
        {
            "params": [p for n, p in model.named_parameters() if "model" not in n],
            "lr": max_lr,
            "weight_decay": weight_decay,
        }
    ]

class CustomCallback(TrainerCallback):
    def __init__(self, train_dataset):
        self.train_dataset = train_dataset

    def on_step_end(self, args, state, control, **kwargs):
        self.train_dataset.step += 1

from transformers.trainer_utils import PredictionOutput

class CustomTrainer(Trainer):
    def get_train_dataloader(self):
        train_dataset = self.train_dataset
        if train_dataset is None:
            raise ValueError("Trainer: training requires a train_dataset.")

        # Use the custom sampler
        sampler = CustomBatchSampler(train_dataset.get_labels(), self.args.train_batch_size)

        return DataLoader(
            train_dataset,
            batch_sampler=sampler,
            collate_fn=self.data_collator,
            num_workers=self.args.dataloader_num_workers,
        )

    def evaluation_loop(self, dataloader, description, prediction_loss_only=None, ignore_keys=None, metric_key_prefix="eval"):

        test_loader = DataLoader(self.eval_dataset, batch_size=config.valid_batch_size,
                                num_workers=4, shuffle=False, pin_memory=True)

        preds = []

        model.eval()

        with torch.no_grad():
            bar = tqdm(enumerate(test_loader), total=len(test_loader))
            for step, data in bar:
                data = {k : v.cuda() for k, v in data.items()}
                batch_size = len(data)
                p = 0

                outputs = self.model(**data)
                p += outputs.logits

                preds.append( p.detach().cpu().numpy() )
                torch.cuda.empty_cache()

        preds = np.concatenate(preds)

        metrics = {f"{metric_key_prefix}_pAUC": pAUC(self.eval_dataset.get_labels(), preds), f"{metric_key_prefix}_loss" : log_loss(self.eval_dataset.get_labels(), preds)}

        # Optionally, wrap in EvalPrediction if necessary
        eval_prediction = EvalPrediction(predictions=preds, label_ids=self.eval_dataset.get_labels())
        eval_prediction.metrics = metrics
        eval_prediction.num_samples = len(preds)

        return eval_prediction

import os

for fold in [3]:

    wandb.init(
        project="ISICCompetition",
        group="experiment-35",
        job_type=f"fold_{fold+1}"
    )

    os.makedirs(f"/kaggle/working/fold_{fold+1}", exist_ok=True)

    training_df = train_df[train_df["fold"] != fold]
    valid_df = train_df[train_df["fold"] == fold]
    full_train_df = pd.concat([training_df])
    os_ = RandomUnderSampler(random_state=42, sampling_strategy=0.01)
    full_train_df, _ = os_.fit_resample(full_train_df, full_train_df["target"].apply(int).values)
    train_ds = ISICDataset(full_train_df, bmu=config.balanced_mixup)
    valid_ds = ISICDataset(valid_df, bmu=False)
    model = ISICModel(config.model_pth).to(config.device)

    optimizer = AdamW(model.parameters(), lr=config.lr)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=int(len(train_ds)/config.train_batch_size), eta_min=1e-5)

    arguments = TrainingArguments(output_dir=f"/kaggle/working/fold_{fold+1}",
                      learning_rate=config.lr,
                      per_device_train_batch_size=config.train_batch_size,
                      per_device_eval_batch_size=config.valid_batch_size,
                      save_strategy="steps",
                      save_steps=int((len(train_ds) / config.train_batch_size) / 4),
                      num_train_epochs=config.epochs,
                      report_to="wandb" if config.wandb else None,
                      logging_steps=10,
                      label_smoothing_factor=0.0,
                      do_eval=True,
                      eval_strategy="steps",
                      eval_steps=int((len(train_ds) / config.train_batch_size) / 4),
                      label_names=["target"],
                      fp16=True,
                      dataloader_num_workers=4,
                      )

    if config.balanced_mixup:
        trainer = CustomTrainer(model=model,
                                train_dataset=train_ds,
                                eval_dataset=valid_ds,
                                args=arguments,
                                optimizers=(optimizer, scheduler),
                            )
    else:
        trainer = Trainer(model=model,
                                train_dataset=train_ds,
                                eval_dataset=valid_ds,
                                args=arguments,
                                optimizers=(optimizer, scheduler),
                            )

    trainer.can_return_loss = True

    trainer.train()

    del model, trainer, optimizer, train_ds
    gc.collect()
    torch.cuda.empty_cache()

    wandb.finish()

